{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a65e9d0a-af99-4bd2-ba9c-a5cbf1a947b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bfebcf8-fad0-41db-9a57-b057b1a1b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "571a7461-12ad-4e43-a8c6-44277b234a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN module without encoder and decoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=69, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(2176, 128)  # Adjust input features based on your image size\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "482a1050-b25c-4b85-b725-df17bd6939e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c767e40f-fb96-4d84-b9f3-4c52b6d4e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_classes = 10\n",
    "learning_rate =0.001\n",
    "batch_size = 50\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce83994b-96e2-41c8-bb05-5575bdbe4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load('images.npy')\n",
    "labels = np.load('./labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3336ccd-7886-4296-9d13-81d42609028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data = torch.from_numpy(images).float()\n",
    "tensor_labels = torch.from_numpy(labels).long()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "test_size = 0.2  \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    tensor_data, tensor_labels, test_size=test_size, random_state=2\n",
    ")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# Create PyTorch DataLoaders\n",
    "batch_size = 50  # Adjust as needed\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e40cd18a-8b7f-4598-a002-3116e78b96ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNN(num_classes=10).to(device)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ca350dc-cfde-4ddc-a2dc-e7f0217d3f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17428\n"
     ]
    }
   ],
   "source": [
    "train_count = len(train_data)\n",
    "test_count = len(test_data)\n",
    "print(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd0252a-de66-40b4-91e7-93faf7383fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(1.1140) Train Accuracy: 0.5817649759008492 Test Accuracy: 0.6414964425063117\n",
      "Epoch: 1 Train Loss: tensor(0.8761) Train Accuracy: 0.6744319485884783 Test Accuracy: 0.6972687629102593\n",
      "Epoch: 2 Train Loss: tensor(0.7944) Train Accuracy: 0.7046132660087217 Test Accuracy: 0.6991048886848749\n",
      "Epoch: 3 Train Loss: tensor(0.7340) Train Accuracy: 0.7287124168005509 Test Accuracy: 0.7399586871700712\n",
      "Epoch: 4 Train Loss: tensor(0.7037) Train Accuracy: 0.7389258664218499 Test Accuracy: 0.7477622217121873\n",
      "Epoch: 5 Train Loss: tensor(0.6654) Train Accuracy: 0.7526968097314666 Test Accuracy: 0.7254991966949736\n",
      "Epoch: 6 Train Loss: tensor(0.6425) Train Accuracy: 0.7608446178563232 Test Accuracy: 0.7553362405324765\n",
      "Epoch: 7 Train Loss: tensor(0.6149) Train Accuracy: 0.7742139086527428 Test Accuracy: 0.7482212531558412\n",
      "Epoch: 8 Train Loss: tensor(0.5945) Train Accuracy: 0.782648611429883 Test Accuracy: 0.7206793665366078\n",
      "Epoch: 9 Train Loss: tensor(0.5704) Train Accuracy: 0.7911406931374799 Test Accuracy: 0.7087445490016067\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "from torch.autograd import Variable\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_dataloader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_dataloader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df72c39a-868b-4ef4-93d2-26640dea7d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [00:02, 100.53it/s]                                                                                                                                  \n",
      "69it [00:00, 276.42it/s]                                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.9362\n",
      "Val Loss : 0.0131\n",
      "Train accuracy : 0.3044\n",
      "Test accuracy : 0.3036\n",
      "Epoch 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [00:02, 107.97it/s]                                                                                                                                  \n",
      "69it [00:00, 251.92it/s]                                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0126\n",
      "Val Loss : 0.0123\n",
      "Train accuracy : 0.3176\n",
      "Test accuracy : 0.3124\n",
      "Epoch 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [00:02, 101.90it/s]                                                                                                                                  \n",
      "69it [00:00, 260.86it/s]                                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0123\n",
      "Val Loss : 0.0120\n",
      "Train accuracy : 0.3162\n",
      "Test accuracy : 0.3105\n",
      "Epoch 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [00:02, 102.88it/s]                                                                                                                                  \n",
      "69it [00:00, 226.64it/s]                                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0120\n",
      "Val Loss : 0.0119\n",
      "Train accuracy : 0.3128\n",
      "Test accuracy : 0.3108\n",
      "Epoch 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [00:02, 106.57it/s]                                                                                                                                  \n",
      "69it [00:00, 244.75it/s]                                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0120\n",
      "Val Loss : 0.0118\n",
      "Train accuracy : 0.3129\n",
      "Test accuracy : 0.3108\n",
      "Epoch 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [00:02, 102.65it/s]                                                                                                                                  \n",
      "69it [00:00, 239.76it/s]                                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0120\n",
      "Val Loss : 0.0118\n",
      "Train accuracy : 0.3126\n",
      "Test accuracy : 0.3110\n",
      "Epoch 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [00:02, 102.90it/s]                                                                                                                                  \n",
      "69it [00:00, 253.89it/s]                                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0119\n",
      "Val Loss : 0.0118\n",
      "Train accuracy : 0.3134\n",
      "Test accuracy : 0.3112\n",
      "Epoch 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [00:02, 105.43it/s]                                                                                                                                  \n",
      "69it [00:00, 230.42it/s]                                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0119\n",
      "Val Loss : 0.0118\n",
      "Train accuracy : 0.3126\n",
      "Test accuracy : 0.3115\n",
      "Epoch 9 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [00:02, 104.11it/s]                                                                                                                                  \n",
      "69it [00:00, 247.63it/s]                                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0119\n",
      "Val Loss : 0.0118\n",
      "Train accuracy : 0.3141\n",
      "Test accuracy : 0.3110\n",
      "Epoch 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [00:02, 98.24it/s]                                                                                                                                   \n",
      "69it [00:00, 177.44it/s]                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0119\n",
      "Val Loss : 0.0118\n",
      "Train accuracy : 0.3135\n",
      "Test accuracy : 0.3112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"In general, using a Variational Autoencoder (VAE) for image classification may not yield competitive results compared to\n",
    "a well-designed Convolutional Neural Network (CNN). The primary purpose of a VAE is to learn a probabilistic latent space \n",
    "representation of input data, which is useful for generative tasks and image reconstruction. However, VAEs are not specifically\n",
    "designed for classification tasks.\"\"\"\n",
    "\n",
    "\"\"\"A Variational Autoencoder (VAE) is a type of generative model in the family of autoencoders. Unlike traditional autoencoders,\n",
    "VAEs are designed to learn a probabilistic representation of input data, typically for the purpose of generating new samples that\n",
    "resemble the training data.\"\"\"\n",
    "\n",
    "'''The provided PyTorch code implements a Convolutional Variational Autoencoder (ConvVAE) for image classification. \n",
    "The ConvVAE architecture consists of an encoder, which progressively reduces the spatial dimensions of the input image\n",
    "through convolutional layers, and a decoder, which reconstructs the original input from the latent space representation.\n",
    "The reparameterization trick introduces stochasticity during training by sampling from the latent space using the mean\n",
    "and log variance. The training process involves minimizing a loss function that combines Binary Cross Entropy (BCE) loss\n",
    "for image reconstruction and Kullback-Leibler Divergence (KLD) loss for regularization. The code includes functions for both\n",
    "training and validation, iterating through epochs and printing the training and validation losses along with accuracy.\n",
    "The model's performance is evaluated on a dataset split into training and testing sets, and a learning rate scheduler is\n",
    "employed to adjust the learning rate during training. While VAEs are known for generative tasks, this code demonstrates their \n",
    "application to a classification task by combining the strengths of both generative and discriminative approaches.'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Define the Variational Autoencoder (VAE) architecture\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, kernel_size=4, latent_dim=256, init_channels=8):\n",
    "        super(ConvVAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=image_channels,\n",
    "            out_channels=init_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_channels,\n",
    "            out_channels=init_channels*2,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_channels*2,\n",
    "            out_channels=init_channels*4,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_channels*4,\n",
    "            out_channels=64,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=2,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for learning representations\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(128, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim, 64)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=64, out_channels=init_channels*8, kernel_size=kernel_size, stride=1, padding=0\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*8, out_channels=init_channels*4, kernel_size=kernel_size, stride=2, padding=1\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*4, out_channels=init_channels*2, kernel_size=kernel_size, stride=2, padding=1\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_channels*2, out_channels=image_channels, kernel_size=kernel_size, stride=2, padding=1\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        sample = mu + (eps * std)\n",
    "        return sample\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        batch, _, _, _ = x.shape\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
    "        hidden = self.fc1(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        log_var = self.fc_log_var(hidden)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        z = self.fc2(z)\n",
    "        z = z.view(batch, 64, 1, 1)  # Adjust the size before decoding\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        reconstruction = torch.sigmoid(self.dec4(x))\n",
    "        return reconstruction, mu, log_var\n",
    "\n",
    "# Function for calculating the final loss of VAE\n",
    "def final_loss(bce_loss, mu, logvar):\n",
    "    BCE = bce_loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "    \n",
    "# Training function\n",
    "def train(model, dataloader, dataset_size, device, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = dataset_size\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(dataset_size/dataloader.batch_size)):\n",
    "        counter += 1\n",
    "        data, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        bce_loss = criterion(reconstruction, data)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        # Get predicted labels from reconstructed images\n",
    "        _, predicted_labels = torch.max(reconstruction, 1)\n",
    "        predicted_labels = predicted_labels.view(-1)\n",
    "        predicted_labels = predicted_labels[:len(labels)]\n",
    "        correct_predictions += (predicted_labels== labels).sum().item()\n",
    "        if i == int(dataset_size/dataloader.batch_size) - 1:\n",
    "            recon_images = reconstruction\n",
    "\n",
    "    train_loss = running_loss / counter\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy,train_loss\n",
    "\n",
    "\n",
    "# validation function\n",
    "def validate(model, dataloader, dataset, device, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = dataset\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(dataset/dataloader.batch_size)):\n",
    "            counter += 1\n",
    "            data, labels = data[0].to(device), data[1].to(device)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "            # Get predicted labels from reconstructed images\n",
    "            _, predicted_labels = torch.max(reconstruction, 1)\n",
    "            predicted_labels = predicted_labels.view(-1)\n",
    "            predicted_labels = predicted_labels[:len(labels)]\n",
    "            correct_predictions += (predicted_labels== labels).sum().item()\n",
    "            if i == int(dataset/dataloader.batch_size) - 1:\n",
    "                recon_images = reconstruction\n",
    "\n",
    "    val_loss = running_loss / counter\n",
    "    val_accuracy = correct_predictions / total_samples\n",
    "    return val_loss, recon_images,val_accuracy\n",
    "\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize VAE model\n",
    "model = ConvVAE().to(device)\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# loading image and label\n",
    "images = np.load('images.npy')\n",
    "labels = np.load('labels.npy')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "tensor_data = torch.from_numpy(images.transpose(0, 3, 1, 2)).float()\n",
    "tensor_labels = torch.from_numpy(labels).long()\n",
    "\n",
    "\n",
    "# Image resizing transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Apply the transformation to the tensor_data\n",
    "tensor_data_resized = torch.stack([transform(img) for img in tensor_data])\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "test_size = 0.2\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    tensor_data_resized, tensor_labels, test_size=test_size, random_state=2\n",
    ")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# Create PyTorch DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# Set up criterion (Mean Squared Error Loss)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "\n",
    "    # Training phase\n",
    "    train_accuracy,train_epoch_loss = train(\n",
    "        model, train_dataloader, len(train_dataset), device, optimizer, criterion\n",
    "    )\n",
    "\n",
    "    # Validation phase\n",
    "    valid_epoch_loss, recon_images,val_accuracy = validate(\n",
    "        model, test_dataloader, len(test_dataset), device, criterion\n",
    "    )\n",
    "\n",
    "    # Store losses in lists\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "\n",
    "     # Print and log results\n",
    "    print(f\"Train Loss : {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss : {valid_epoch_loss:.4f}\")\n",
    "    print(f\"Train accuracy : {train_accuracy:.4f}\")\n",
    "    print(f\"Test accuracy : {val_accuracy:.4f}\")\n",
    "\n",
    "    # Adjust learning rate\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b1643d-699d-43f5-b3a3-bb9e92cf301f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
